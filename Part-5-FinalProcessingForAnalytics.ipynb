{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d990ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adaff757",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        config('spark.ui.port','0').\\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse').\\\n",
    "        config('spark.shuffle.useOldFetchProtocol','true').\\\n",
    "        enableHiveSupport().\\\n",
    "        master('yarn').\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0bce3",
   "metadata": {},
   "source": [
    "## Associating points to the grades in order to calculate the Loan Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bd940ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.unacceptable_rated_pts\",0)\n",
    "spark.conf.set(\"spark.sql.very_bad_rated_pts\",100)\n",
    "spark.conf.set(\"spark.sql.bad_rated_pts\",250)\n",
    "spark.conf.set(\"spark.sql.good_rated_pts\",500)\n",
    "spark.conf.set(\"spark.sql.very_good_rated_pts\",650)\n",
    "spark.conf.set(\"spark.sql.excellent_rated_pts\",800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ff0af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.unacceptable_grade_pts\",750)\n",
    "spark.conf.set(\"spark.sql.very_bad_grade_pts\",1000)\n",
    "spark.conf.set(\"spark.sql.bad_grade_pts\",1500)\n",
    "spark.conf.set(\"spark.sql.good_grade_pts\",2000)\n",
    "spark.conf.set(\"spark.sql.very_good_grade_pts\",2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc323b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_customer_data_final_df = spark.read.csv(\"/user/itv015278/lendingclubproject/bad_data/customer_data_final\" , header = True , inferSchema = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc4c9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_customer_data_final_df.createOrReplaceTempView(\"bad_data_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaa7e412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE itv015278_lending_club\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202504d6",
   "metadata": {},
   "source": [
    "## Loan Score Calculation Criteria 1: Payment History(ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30282a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_history_df = spark.sql(\"\"\"\n",
    "select c.member_id , case when p.last_payment_amount < (c.monthly_installment * 0.5) then ${spark.sql.very_bad_rated_pts}\n",
    "                          when p.last_payment_amount >= (c.monthly_installment * 0.5) and p.last_payment_amount < c.monthly_installment then ${spark.sql.bad_rated_pts}\n",
    "                          when p.last_payment_amount = c.monthly_installment then ${spark.sql.good_rated_pts}\n",
    "                          when p.last_payment_amount > (c.monthly_installment) and p.last_payment_amount < (c.monthly_installment * 1.5) then ${spark.sql.very_good_rated_pts}\n",
    "                          when p.last_payment_amount > (c.monthly_installment * 1.50) then ${spark.sql.excellent_rated_pts}\n",
    "                          else ${spark.sql.unacceptable_rated_pts}\n",
    "                          end as last_payment_points ,\n",
    "                      case when p.total_payment_received >= (c.funded_amount * 0.50) then ${spark.sql.very_good_rated_pts}\n",
    "                           when p.total_payment_received < (c.funded_amount * 0.50) and p.total_payment_received > 0 then ${spark.sql.good_rated_pts}\n",
    "                           when p.total_payment_received = 0 or p.total_payment_received is NULL then ${spark.sql.unacceptable_rated_pts}\n",
    "                            end as total_payment_points\n",
    "                          from loans_repayments p\n",
    "                          inner join loans c on c.loan_id = p.loan_id\n",
    "                          left join bad_data_customer bdc on bdc.member_id = c.member_id\n",
    "                          where bdc.member_id is NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb000bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_history_df.createOrReplaceTempView(\"ph_pts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20885a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_default_history = spark.sql(\"\"\"\n",
    "select p.* ,\n",
    "CASE WHEN d.delinq_2yrs = 0 THEN ${spark.sql.excellent_rated_pts}\n",
    "     when d.delinq_2yrs BETWEEN 1 and 2 then ${spark.sql.bad_rated_pts}\n",
    "     when d.delinq_2yrs between 3 and 5 then ${spark.sql.very_bad_rated_pts}\n",
    "     when d.delinq_2yrs > 5 OR d.delinq_2yrs is NULL then ${spark.sql.unacceptable_rated_pts}\n",
    "     END as delinq_points ,\n",
    "CASE WHEN l.public_record = 0 THEN ${spark.sql.excellent_rated_pts}\n",
    "     when l.public_record BETWEEN 1 and 2 then ${spark.sql.bad_rated_pts}\n",
    "     when l.public_record between 3 and 5 then ${spark.sql.very_bad_rated_pts}\n",
    "     when l.public_record > 5 OR l.public_record is NULL then ${spark.sql.unacceptable_rated_pts}\n",
    "     END as public_record_points ,\n",
    "CASE WHEN l.public_record_bankruptcies = 0 THEN ${spark.sql.excellent_rated_pts}\n",
    "     when l.public_record_bankruptcies BETWEEN 1 and 2 then ${spark.sql.bad_rated_pts}\n",
    "     when l.public_record_bankruptcies between 3 and 5 then ${spark.sql.very_bad_rated_pts}\n",
    "     when l.public_record_bankruptcies > 5 OR l.public_record_bankruptcies is NULL then ${spark.sql.unacceptable_rated_pts}\n",
    "     END as public_record_bankruptcies_pts ,\n",
    "CASE WHEN l.inquiries_last_6months = 0 THEN ${spark.sql.excellent_rated_pts}\n",
    "     when l.inquiries_last_6months BETWEEN 1 and 2 then ${spark.sql.bad_rated_pts}\n",
    "     when l.inquiries_last_6months BETWEEN 1 and 2 then ${spark.sql.bad_rated_pts}\n",
    "     when l.inquiries_last_6months between 3 and 5 then ${spark.sql.very_bad_rated_pts}\n",
    "     when l.inquiries_last_6months > 5 OR l.inquiries_last_6months is NULL then ${spark.sql.unacceptable_rated_pts}\n",
    "     END as enquiry_points \n",
    "from loans_defaulters_detail_rec_enq_new l\n",
    "inner join loans_defaulters_delinq_new d on l.member_id = d.member_id\n",
    "inner join ph_pts p on p.member_id = l.member_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8caecf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_default_history.createOrReplaceTempView(\"ldh_ph_pts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bc37b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_ldh_ph_df = spark.sql(\"select ldef.*, \\\n",
    "   CASE \\\n",
    "   WHEN LOWER(l.loan_status) LIKE '%fully paid%' THEN ${spark.sql.excellent_rated_pts} \\\n",
    "   WHEN LOWER(l.loan_status) LIKE '%current%' THEN ${spark.sql.good_rated_pts} \\\n",
    "   WHEN LOWER(l.loan_status) LIKE '%in grace period%' THEN ${spark.sql.bad_rated_pts} \\\n",
    "   WHEN LOWER(l.loan_status) LIKE '%late (16-30 days)%' OR LOWER(l.loan_status) LIKE '%late (31-120 days)%' THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "   WHEN LOWER(l.loan_status) LIKE '%charged off%' THEN ${spark.sql.unacceptable_rated_pts} \\\n",
    "   else ${spark.sql.unacceptable_rated_pts} \\\n",
    "   END AS loan_status_pts, \\\n",
    "   CASE \\\n",
    "   WHEN LOWER(a.home_ownership) LIKE '%own' THEN ${spark.sql.excellent_rated_pts} \\\n",
    "   WHEN LOWER(a.home_ownership) LIKE '%rent' THEN ${spark.sql.good_rated_pts} \\\n",
    "   WHEN LOWER(a.home_ownership) LIKE '%mortgage' THEN ${spark.sql.bad_rated_pts} \\\n",
    "   WHEN LOWER(a.home_ownership) LIKE '%any' OR LOWER(a.home_ownership) IS NULL THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "   END AS home_pts, \\\n",
    "   CASE \\\n",
    "   WHEN l.funded_amount <= (a.total_high_credit_limit * 0.10) THEN ${spark.sql.excellent_rated_pts} \\\n",
    "   WHEN l.funded_amount > (a.total_high_credit_limit * 0.10) AND l.funded_amount <= (a.total_high_credit_limit * 0.20) THEN ${spark.sql.very_good_rated_pts} \\\n",
    "   WHEN l.funded_amount > (a.total_high_credit_limit * 0.20) AND l.funded_amount <= (a.total_high_credit_limit * 0.30) THEN ${spark.sql.good_rated_pts} \\\n",
    "   WHEN l.funded_amount > (a.total_high_credit_limit * 0.30) AND l.funded_amount <= (a.total_high_credit_limit * 0.50) THEN ${spark.sql.bad_rated_pts} \\\n",
    "   WHEN l.funded_amount > (a.total_high_credit_limit * 0.50) AND l.funded_amount <= (a.total_high_credit_limit * 0.70) THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "   WHEN l.funded_amount > (a.total_high_credit_limit * 0.70) THEN ${spark.sql.unacceptable_rated_pts} \\\n",
    "   else ${spark.sql.unacceptable_rated_pts} \\\n",
    "   END AS credit_limit_pts, \\\n",
    "   CASE \\\n",
    "   WHEN (a.grade) = 'A' and (a.sub_grade)='A1' THEN ${spark.sql.excellent_rated_pts} \\\n",
    "   WHEN (a.grade) = 'A' and (a.sub_grade)='A2' THEN (${spark.sql.excellent_rated_pts} * 0.95) \\\n",
    "   WHEN (a.grade) = 'A' and (a.sub_grade)='A3' THEN (${spark.sql.excellent_rated_pts} * 0.90) \\\n",
    "   WHEN (a.grade) = 'A' and (a.sub_grade)='A4' THEN (${spark.sql.excellent_rated_pts} * 0.85) \\\n",
    "   WHEN (a.grade) = 'A' and (a.sub_grade)='A5' THEN (${spark.sql.excellent_rated_pts} * 0.80) \\\n",
    "   WHEN (a.grade) = 'B' and (a.sub_grade)='B1' THEN (${spark.sql.very_good_rated_pts}) \\\n",
    "   WHEN (a.grade) = 'B' and (a.sub_grade)='B2' THEN (${spark.sql.very_good_rated_pts} * 0.95) \\\n",
    "   WHEN (a.grade) = 'B' and (a.sub_grade)='B3' THEN (${spark.sql.very_good_rated_pts} * 0.90) \\\n",
    "   WHEN (a.grade) = 'B' and (a.sub_grade)='B4' THEN (${spark.sql.very_good_rated_pts} * 0.85) \\\n",
    "   WHEN (a.grade) = 'B' and (a.sub_grade)='B5' THEN (${spark.sql.very_good_rated_pts} * 0.80) \\\n",
    "   WHEN (a.grade) = 'C' and (a.sub_grade)='C1' THEN (${spark.sql.good_rated_pts}) \\\n",
    "   WHEN (a.grade) = 'C' and (a.sub_grade)='C2' THEN (${spark.sql.good_rated_pts} * 0.95) \\\n",
    "   WHEN (a.grade) = 'C' and (a.sub_grade)='C3' THEN (${spark.sql.good_rated_pts} * 0.90) \\\n",
    "   WHEN (a.grade) = 'C' and (a.sub_grade)='C4' THEN (${spark.sql.good_rated_pts} * 0.85) \\\n",
    "   WHEN (a.grade) = 'C' and (a.sub_grade)='C5' THEN (${spark.sql.good_rated_pts} * 0.80) \\\n",
    "   WHEN (a.grade) = 'D' and (a.sub_grade)='D1' THEN (${spark.sql.bad_rated_pts}) \\\n",
    "   WHEN (a.grade) = 'D' and (a.sub_grade)='D2' THEN (${spark.sql.bad_rated_pts} * 0.95) \\\n",
    "   WHEN (a.grade) = 'D' and (a.sub_grade)='D3' THEN (${spark.sql.bad_rated_pts} * 0.90) \\\n",
    "   WHEN (a.grade) = 'D' and (a.sub_grade)='D4' THEN (${spark.sql.bad_rated_pts} * 0.85) \\\n",
    "   WHEN (a.grade) = 'D' and (a.sub_grade)='D5' THEN (${spark.sql.bad_rated_pts} * 0.80) \\\n",
    "   WHEN (a.grade) = 'E' and (a.sub_grade)='E1' THEN (${spark.sql.very_bad_rated_pts}) \\\n",
    "   WHEN (a.grade) = 'E' and (a.sub_grade)='E2' THEN (${spark.sql.very_bad_rated_pts} * 0.95) \\\n",
    "   WHEN (a.grade) = 'E' and (a.sub_grade)='E3' THEN (${spark.sql.very_bad_rated_pts} * 0.90) \\\n",
    "   WHEN (a.grade) = 'E' and (a.sub_grade)='E4' THEN (${spark.sql.very_bad_rated_pts} * 0.85) \\\n",
    "   WHEN (a.grade) = 'E' and (a.sub_grade)='E5' THEN (${spark.sql.very_bad_rated_pts} * 0.80) \\\n",
    "   WHEN (a.grade) in ('F', 'G') THEN (${spark.sql.unacceptable_rated_pts}) \\\n",
    "   END AS grade_pts \\\n",
    "   FROM ldh_ph_pts ldef \\\n",
    "   INNER JOIN loans l ON ldef.member_id = l.member_id \\\n",
    "   INNER JOIN customers_new a ON a.member_id = ldef.member_id  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c6532",
   "metadata": {},
   "source": [
    "## Loan Score Calculation Criteria 2: Loan Defaulters History(ldh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "899b6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_ldh_ph_df.createOrReplaceTempView(\"fh_ldh_ph_pts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11bf85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score = spark.sql(\"SELECT member_id, \\\n",
    "((last_payment_points+total_payment_points)*0.20) as payment_history_points, \\\n",
    "((delinq_points + public_record_points + public_record_bankruptcies_pts + enquiry_points) * 0.45) as defaulters_history_points, \\\n",
    "((loan_status_pts + home_pts + credit_limit_pts + grade_pts)*0.35) as financial_health_points \\\n",
    "FROM fh_ldh_ph_pts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e861edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loan_score = loan_score.withColumn('loan_score', loan_score.payment_history_points + loan_score.defaulters_history_points + loan_score.financial_health_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74f00980",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loan_score.createOrReplaceTempView(\"loan_score_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52317e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_final = spark.sql(\"select ls.*, \\\n",
    "case \\\n",
    "WHEN loan_score > ${spark.sql.very_good_grade_pts} THEN 'A' \\\n",
    "WHEN loan_score <= ${spark.sql.very_good_grade_pts} AND loan_score > ${spark.sql.good_grade_pts} THEN 'B' \\\n",
    "WHEN loan_score <= ${spark.sql.good_grade_pts} AND loan_score > ${spark.sql.bad_grade_pts} THEN 'C' \\\n",
    "WHEN loan_score <= ${spark.sql.bad_grade_pts} AND loan_score  > ${spark.sql.very_bad_grade_pts} THEN 'D' \\\n",
    "WHEN loan_score <= ${spark.sql.very_bad_grade_pts} AND loan_score > ${spark.sql.unacceptable_grade_pts} THEN 'E'  \\\n",
    "WHEN loan_score <= ${spark.sql.unacceptable_grade_pts} THEN 'F' \\\n",
    "end as loan_final_grade \\\n",
    "from loan_score_eval ls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a03a34d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_score_final.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf8bdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_final.write \\\n",
    ".format(\"parquet\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"path\", \"/user/itv015278/lendingclubproject/processed/loan_score\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468be98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
